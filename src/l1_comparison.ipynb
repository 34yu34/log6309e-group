{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1W1rYOtVhhhFBT7L-p7vWyVU8ZysqQNdG","authorship_tag":"ABX9TyMjKD4NEEU/0Z1QsI79BvdI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9DVvsknx8zvI","executionInfo":{"status":"ok","timestamp":1697525934335,"user_tz":240,"elapsed":269,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"34338a54-da19-4fa2-eb58-9e252dd34771"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/My Drive/Code/GroupProjectDevops/log6309e-group/src'\n","/content/drive/My Drive/Code/GroupProjectDevops/log6309e-group/src\n"]}],"source":["%cd drive/My Drive/Code/GroupProjectDevops/log6309e-group/src"]},{"cell_type":"code","source":["from splitdatabgl import split_bgl\n","from extensions.stat_ranking import ModelData\n","from models.traditional import SVM\n","from models.traditional import DecisionTree\n","from models.traditional import LR\n","from models.MLP import MLP\n","from models.MLP_l1 import MLP_L1\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"YshFcfID9SlE","executionInfo":{"status":"ok","timestamp":1697526042941,"user_tz":240,"elapsed":1305,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["data = np.load('../data/BGL/BGL-log.splitted.npz')\n","train_x = data['x_train']\n","train_y = data['y_train']\n","test_x = data['x_test']\n","test_y = data['y_test']\n","\n","train_l1_x = data['x_train_l1']\n","train_l1_y = data['y_train_l1']\n","test_l1_x = data['x_test_l1']\n","test_l1_y = data['y_test_l1']"],"metadata":{"id":"lUsLDeS69Tfv","executionInfo":{"status":"ok","timestamp":1697525934616,"user_tz":240,"elapsed":8,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def lr_model_eval(x_train, y_train, x_test, y_test):\n","    lr = LR(max_iter=100000)\n","    lr.fit(train_x, train_y)\n","    return lr.evaluate(test_x, test_y)\n","\n","def decision_tree_model_eval(x_train, y_train, x_test, y_test):\n","    decision_tree = DecisionTree()\n","    decision_tree.fit(train_x, train_y)\n","    return decision_tree.evaluate(test_x, test_y)\n","\n","def SVM_model_eval(x_train, y_train, x_test, y_test):\n","    svm = SVM(train_x, train_y, test_x, test_y)\n","    return svm.evaluate()"],"metadata":{"id":"HdzGBRmR9pnX","executionInfo":{"status":"ok","timestamp":1697525934617,"user_tz":240,"elapsed":8,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["lr_eval = lr_model_eval(train_x, train_y, test_x, test_y)\n","lr_l1_eval = lr_model_eval(train_l1_x, train_l1_y, test_l1_x, test_l1_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dz7kLO489yu9","executionInfo":{"status":"ok","timestamp":1697525950926,"user_tz":240,"elapsed":16317,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"eaca528e-f04e-4241-b6d4-3fe7297098bb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["====== Model summary ======\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["====== Evaluation summary ======\n","Precision: 0.957, recall: 0.905, F1-measure: 0.931\n","\n","====== Model summary ======\n","====== Evaluation summary ======\n","Precision: 0.957, recall: 0.905, F1-measure: 0.931\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"code","source":["decision_tree_eval = decision_tree_model_eval(train_x, train_y, test_x, test_y)\n","decision_tree_l1_eval = decision_tree_model_eval(train_l1_x, train_l1_y, test_l1_x, test_l1_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"soJROQ5K-bWb","executionInfo":{"status":"ok","timestamp":1697525950926,"user_tz":240,"elapsed":22,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"b23c5fb4-a2e2-45e5-e369-84fd5f54ffeb"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["====== Model summary ======\n","====== Evaluation summary ======\n","Precision: 0.985, recall: 0.892, F1-measure: 0.936\n","\n","====== Model summary ======\n","====== Evaluation summary ======\n","Precision: 0.985, recall: 0.892, F1-measure: 0.936\n","\n"]}]},{"cell_type":"code","source":["SVM_eval = SVM_model_eval(train_x, train_y, test_x, test_y)\n","SVM_l1_eval = SVM_model_eval(train_l1_x, train_l1_y, test_l1_x, test_l1_y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbBUABcD-bwQ","executionInfo":{"status":"ok","timestamp":1697525953972,"user_tz":240,"elapsed":3064,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"772fb91f-61bd-4270-a9b7-5bd5a624de56"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","average:  0.12162162162162163 0.9 0.21428571428571433\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","Precision: 0.122, recall: 0.900, F1-measure: 0.214\n","\n","average:  0.12162162162162163 0.9 0.21428571428571433\n"]}]},{"cell_type":"code","source":["mlp = MLP('../data/BGL/BGL-log.splitted.npz')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4RUTWHwpi3lg","executionInfo":{"status":"ok","timestamp":1697525953973,"user_tz":240,"elapsed":22,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"24b7bcd2-3952-4c93-e86b-4b4c3fd42045"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(537, 349)\n","(537,)\n","positive: 258\n","ratio: 2.0813953488372094\n","Net(\n","  (hidden1): Linear(in_features=349, out_features=200, bias=True)\n","  (hidden2): Linear(in_features=200, out_features=200, bias=True)\n","  (output): Linear(in_features=200, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["loss_list, precision_list, recall_list, f1_list, accuracy_list = mlp.train_eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wGa-n8Gj1Y1","executionInfo":{"status":"ok","timestamp":1697526011442,"user_tz":240,"elapsed":57485,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"04d56447-bc72-4580-ce70-20e4a99a0f71"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["itr: 0, loss: 0.689897\n","Testset: Precision: 0.548, recall: 1.000, F1-measure: 0.708, acc:0.548\n","\n","itr: 10, loss: 0.586373\n","Testset: Precision: 0.548, recall: 1.000, F1-measure: 0.708, acc:0.548\n","\n","itr: 20, loss: 0.401653\n","Testset: Precision: 0.824, recall: 0.824, F1-measure: 0.824, acc:0.807\n","\n","itr: 30, loss: 0.236900\n","Testset: Precision: 0.815, recall: 0.892, F1-measure: 0.852, acc:0.830\n","\n","itr: 40, loss: 0.165798\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 50, loss: 0.147184\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 60, loss: 0.130940\n","Testset: Precision: 0.827, recall: 0.838, F1-measure: 0.832, acc:0.815\n","\n","itr: 70, loss: 0.113888\n","Testset: Precision: 0.813, recall: 0.824, F1-measure: 0.819, acc:0.800\n","\n","itr: 80, loss: 0.102792\n","Testset: Precision: 0.827, recall: 0.838, F1-measure: 0.832, acc:0.815\n","\n","itr: 90, loss: 0.101174\n","Testset: Precision: 0.840, recall: 0.851, F1-measure: 0.846, acc:0.830\n","\n","itr: 100, loss: 0.095388\n","Testset: Precision: 0.813, recall: 0.824, F1-measure: 0.819, acc:0.800\n","\n","itr: 110, loss: 0.087907\n","Testset: Precision: 0.849, recall: 0.838, F1-measure: 0.844, acc:0.830\n","\n","itr: 120, loss: 0.087233\n","Testset: Precision: 0.836, recall: 0.824, F1-measure: 0.830, acc:0.815\n","\n","itr: 130, loss: 0.085858\n","Testset: Precision: 0.840, recall: 0.851, F1-measure: 0.846, acc:0.830\n","\n","itr: 140, loss: 0.083866\n","Testset: Precision: 0.836, recall: 0.824, F1-measure: 0.830, acc:0.815\n","\n","itr: 150, loss: 0.082139\n","Testset: Precision: 0.840, recall: 0.851, F1-measure: 0.846, acc:0.830\n","\n","itr: 160, loss: 0.082723\n","Testset: Precision: 0.838, recall: 0.838, F1-measure: 0.838, acc:0.822\n","\n","itr: 170, loss: 0.079392\n","Testset: Precision: 0.840, recall: 0.851, F1-measure: 0.846, acc:0.830\n","\n","itr: 180, loss: 0.080733\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 190, loss: 0.078782\n","Testset: Precision: 0.824, recall: 0.824, F1-measure: 0.824, acc:0.807\n","\n","itr: 200, loss: 0.075656\n","Testset: Precision: 0.827, recall: 0.838, F1-measure: 0.832, acc:0.815\n","\n","itr: 210, loss: 0.077150\n","Testset: Precision: 0.838, recall: 0.838, F1-measure: 0.838, acc:0.822\n","\n","itr: 220, loss: 0.076873\n","Testset: Precision: 0.882, recall: 0.811, F1-measure: 0.845, acc:0.837\n","\n","itr: 230, loss: 0.076266\n","Testset: Precision: 0.805, recall: 0.838, F1-measure: 0.821, acc:0.800\n","\n","itr: 240, loss: 0.074024\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 250, loss: 0.072118\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 260, loss: 0.070347\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 270, loss: 0.069283\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 280, loss: 0.067856\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 290, loss: 0.068260\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 300, loss: 0.067437\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 310, loss: 0.067097\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 320, loss: 0.066722\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 330, loss: 0.066205\n","Testset: Precision: 0.805, recall: 0.838, F1-measure: 0.821, acc:0.800\n","\n","itr: 340, loss: 0.065729\n","Testset: Precision: 0.805, recall: 0.838, F1-measure: 0.821, acc:0.800\n","\n","itr: 350, loss: 0.065320\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 360, loss: 0.065342\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 370, loss: 0.065704\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 380, loss: 0.064907\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 390, loss: 0.064423\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 400, loss: 0.064339\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 410, loss: 0.064140\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 420, loss: 0.063918\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 430, loss: 0.063703\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 440, loss: 0.064421\n","Testset: Precision: 0.805, recall: 0.838, F1-measure: 0.821, acc:0.800\n","\n","itr: 450, loss: 0.064741\n","Testset: Precision: 0.795, recall: 0.838, F1-measure: 0.816, acc:0.793\n","\n","itr: 460, loss: 0.063924\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 470, loss: 0.063445\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 480, loss: 0.074404\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 490, loss: 0.072376\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 500, loss: 0.071774\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 510, loss: 0.106729\n","Testset: Precision: 0.894, recall: 0.797, F1-measure: 0.843, acc:0.837\n","\n","itr: 520, loss: 0.079221\n","Testset: Precision: 0.802, recall: 0.878, F1-measure: 0.839, acc:0.815\n","\n","itr: 530, loss: 0.081599\n","Testset: Precision: 0.802, recall: 0.878, F1-measure: 0.839, acc:0.815\n","\n","itr: 540, loss: 0.075272\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 550, loss: 0.071516\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 560, loss: 0.071341\n","Testset: Precision: 0.870, recall: 0.811, F1-measure: 0.839, acc:0.830\n","\n","itr: 570, loss: 0.069309\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 580, loss: 0.078192\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 590, loss: 0.072181\n","Testset: Precision: 0.881, recall: 0.797, F1-measure: 0.837, acc:0.830\n","\n","itr: 600, loss: 0.069984\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 610, loss: 0.069114\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 620, loss: 0.068665\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 630, loss: 0.068281\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 640, loss: 0.069369\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 650, loss: 0.068195\n","Testset: Precision: 0.787, recall: 0.851, F1-measure: 0.818, acc:0.793\n","\n","itr: 660, loss: 0.067521\n","Testset: Precision: 0.787, recall: 0.851, F1-measure: 0.818, acc:0.793\n","\n","itr: 670, loss: 0.067282\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 680, loss: 0.066959\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 690, loss: 0.066857\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 700, loss: 0.066544\n","Testset: Precision: 0.797, recall: 0.851, F1-measure: 0.824, acc:0.800\n","\n","itr: 710, loss: 0.066233\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 720, loss: 0.065985\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 730, loss: 0.065736\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 740, loss: 0.065411\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 750, loss: 0.065067\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 760, loss: 0.064834\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 770, loss: 0.064601\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 780, loss: 0.064289\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 790, loss: 0.063999\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 800, loss: 0.063736\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 810, loss: 0.067059\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 820, loss: 0.066257\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 830, loss: 0.063637\n","Testset: Precision: 0.870, recall: 0.811, F1-measure: 0.839, acc:0.830\n","\n","itr: 840, loss: 0.063262\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 850, loss: 0.063013\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 860, loss: 0.062706\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 870, loss: 0.062474\n","Testset: Precision: 0.790, recall: 0.865, F1-measure: 0.826, acc:0.800\n","\n","itr: 880, loss: 0.093031\n","Testset: Precision: 0.787, recall: 0.851, F1-measure: 0.818, acc:0.793\n","\n","itr: 890, loss: 0.083174\n","Testset: Precision: 0.919, recall: 0.770, F1-measure: 0.838, acc:0.837\n","\n","itr: 900, loss: 0.084155\n","Testset: Precision: 0.896, recall: 0.811, F1-measure: 0.851, acc:0.844\n","\n","itr: 910, loss: 0.168331\n","Testset: Precision: 0.808, recall: 0.851, F1-measure: 0.829, acc:0.807\n","\n","itr: 920, loss: 0.136490\n","Testset: Precision: 0.849, recall: 0.838, F1-measure: 0.844, acc:0.830\n","\n","itr: 930, loss: 0.123224\n","Testset: Precision: 0.861, recall: 0.838, F1-measure: 0.849, acc:0.837\n","\n","itr: 940, loss: 0.115099\n","Testset: Precision: 0.859, recall: 0.824, F1-measure: 0.841, acc:0.830\n","\n","itr: 950, loss: 0.117174\n","Testset: Precision: 0.825, recall: 0.892, F1-measure: 0.857, acc:0.837\n","\n","itr: 960, loss: 0.132668\n","Testset: Precision: 0.802, recall: 0.932, F1-measure: 0.863, acc:0.837\n","\n","itr: 970, loss: 0.116774\n","Testset: Precision: 0.819, recall: 0.919, F1-measure: 0.866, acc:0.844\n","\n","itr: 980, loss: 0.100177\n","Testset: Precision: 0.815, recall: 0.892, F1-measure: 0.852, acc:0.830\n","\n","itr: 990, loss: 0.103651\n","Testset: Precision: 0.861, recall: 0.838, F1-measure: 0.849, acc:0.837\n","\n","itr: 1000, loss: 0.099784\n","Testset: Precision: 0.886, recall: 0.838, F1-measure: 0.861, acc:0.852\n","\n","itr: 1010, loss: 0.120546\n","Testset: Precision: 0.807, recall: 0.905, F1-measure: 0.854, acc:0.830\n","\n","itr: 1020, loss: 0.106932\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 1030, loss: 0.126714\n","Testset: Precision: 0.805, recall: 0.838, F1-measure: 0.821, acc:0.800\n","\n","itr: 1040, loss: 0.110826\n","Testset: Precision: 0.900, recall: 0.851, F1-measure: 0.875, acc:0.867\n","\n","itr: 1050, loss: 0.100953\n","Testset: Precision: 0.827, recall: 0.905, F1-measure: 0.865, acc:0.844\n","\n","itr: 1060, loss: 0.103550\n","Testset: Precision: 0.825, recall: 0.892, F1-measure: 0.857, acc:0.837\n","\n","itr: 1070, loss: 0.095536\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1080, loss: 0.095678\n","Testset: Precision: 0.815, recall: 0.892, F1-measure: 0.852, acc:0.830\n","\n","itr: 1090, loss: 0.094938\n","Testset: Precision: 0.815, recall: 0.892, F1-measure: 0.852, acc:0.830\n","\n","itr: 1100, loss: 0.094148\n","Testset: Precision: 0.825, recall: 0.892, F1-measure: 0.857, acc:0.837\n","\n","itr: 1110, loss: 0.095116\n","Testset: Precision: 0.817, recall: 0.905, F1-measure: 0.859, acc:0.837\n","\n","itr: 1120, loss: 0.092058\n","Testset: Precision: 0.827, recall: 0.905, F1-measure: 0.865, acc:0.844\n","\n","itr: 1130, loss: 0.092771\n","Testset: Precision: 0.835, recall: 0.892, F1-measure: 0.863, acc:0.844\n","\n","itr: 1140, loss: 0.091817\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1150, loss: 0.090886\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1160, loss: 0.090030\n","Testset: Precision: 0.829, recall: 0.851, F1-measure: 0.840, acc:0.822\n","\n","itr: 1170, loss: 0.089616\n","Testset: Precision: 0.829, recall: 0.851, F1-measure: 0.840, acc:0.822\n","\n","itr: 1180, loss: 0.089344\n","Testset: Precision: 0.829, recall: 0.851, F1-measure: 0.840, acc:0.822\n","\n","itr: 1190, loss: 0.089133\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1200, loss: 0.088961\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1210, loss: 0.088808\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1220, loss: 0.088668\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1230, loss: 0.088538\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1240, loss: 0.088135\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1250, loss: 0.088290\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1260, loss: 0.087980\n","Testset: Precision: 0.842, recall: 0.865, F1-measure: 0.853, acc:0.837\n","\n","itr: 1270, loss: 0.095044\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1280, loss: 0.088105\n","Testset: Precision: 0.922, recall: 0.797, F1-measure: 0.855, acc:0.852\n","\n","itr: 1290, loss: 0.089291\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1300, loss: 0.088919\n","Testset: Precision: 0.922, recall: 0.797, F1-measure: 0.855, acc:0.852\n","\n","itr: 1310, loss: 0.088817\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1320, loss: 0.088671\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1330, loss: 0.088556\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1340, loss: 0.088408\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1350, loss: 0.088336\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1360, loss: 0.088268\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1370, loss: 0.088207\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1380, loss: 0.088149\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1390, loss: 0.088092\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1400, loss: 0.088023\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1410, loss: 0.087924\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1420, loss: 0.088174\n","Testset: Precision: 0.805, recall: 0.892, F1-measure: 0.846, acc:0.822\n","\n","itr: 1430, loss: 0.088318\n","Testset: Precision: 0.906, recall: 0.784, F1-measure: 0.841, acc:0.837\n","\n","itr: 1440, loss: 0.087936\n","Testset: Precision: 0.818, recall: 0.851, F1-measure: 0.834, acc:0.815\n","\n","itr: 1450, loss: 0.087880\n","Testset: Precision: 0.829, recall: 0.851, F1-measure: 0.840, acc:0.822\n","\n","itr: 1460, loss: 0.087754\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1470, loss: 0.087671\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1480, loss: 0.087602\n","Testset: Precision: 0.831, recall: 0.865, F1-measure: 0.848, acc:0.830\n","\n","itr: 1490, loss: 0.086333\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1500, loss: 0.087553\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1510, loss: 0.087569\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1520, loss: 0.093667\n","Testset: Precision: 0.921, recall: 0.784, F1-measure: 0.847, acc:0.844\n","\n","itr: 1530, loss: 0.094819\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1540, loss: 0.096775\n","Testset: Precision: 0.909, recall: 0.811, F1-measure: 0.857, acc:0.852\n","\n","itr: 1550, loss: 0.091075\n","Testset: Precision: 0.908, recall: 0.797, F1-measure: 0.849, acc:0.844\n","\n","itr: 1560, loss: 0.089753\n","Testset: Precision: 0.827, recall: 0.838, F1-measure: 0.832, acc:0.815\n","\n","itr: 1570, loss: 0.086830\n","Testset: Precision: 0.827, recall: 0.838, F1-measure: 0.832, acc:0.815\n","\n","itr: 1580, loss: 0.104594\n","Testset: Precision: 0.906, recall: 0.784, F1-measure: 0.841, acc:0.837\n","\n","itr: 1590, loss: 0.098650\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1600, loss: 0.091256\n","Testset: Precision: 0.906, recall: 0.784, F1-measure: 0.841, acc:0.837\n","\n","itr: 1610, loss: 0.090702\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 1620, loss: 0.088778\n","Testset: Precision: 0.812, recall: 0.878, F1-measure: 0.844, acc:0.822\n","\n","itr: 1630, loss: 0.091280\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1640, loss: 0.090721\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1650, loss: 0.090201\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1660, loss: 0.091564\n","Testset: Precision: 0.882, recall: 0.811, F1-measure: 0.845, acc:0.837\n","\n","itr: 1670, loss: 0.090357\n","Testset: Precision: 0.882, recall: 0.811, F1-measure: 0.845, acc:0.837\n","\n","itr: 1680, loss: 0.088752\n","Testset: Precision: 0.910, recall: 0.824, F1-measure: 0.865, acc:0.859\n","\n","itr: 1690, loss: 0.089019\n","Testset: Precision: 0.810, recall: 0.865, F1-measure: 0.837, acc:0.815\n","\n","itr: 1700, loss: 0.089737\n","Testset: Precision: 0.812, recall: 0.878, F1-measure: 0.844, acc:0.822\n","\n","itr: 1710, loss: 0.089147\n","Testset: Precision: 0.800, recall: 0.865, F1-measure: 0.831, acc:0.807\n","\n","itr: 1720, loss: 0.091387\n","Testset: Precision: 0.833, recall: 0.878, F1-measure: 0.855, acc:0.837\n","\n","itr: 1730, loss: 0.092195\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1740, loss: 0.088354\n","Testset: Precision: 0.835, recall: 0.892, F1-measure: 0.863, acc:0.844\n","\n","itr: 1750, loss: 0.173632\n","Testset: Precision: 0.881, recall: 0.797, F1-measure: 0.837, acc:0.830\n","\n","itr: 1760, loss: 0.102001\n","Testset: Precision: 0.881, recall: 0.797, F1-measure: 0.837, acc:0.830\n","\n","itr: 1770, loss: 0.101062\n","Testset: Precision: 0.921, recall: 0.784, F1-measure: 0.847, acc:0.844\n","\n","itr: 1780, loss: 0.094111\n","Testset: Precision: 0.802, recall: 0.878, F1-measure: 0.839, acc:0.815\n","\n","itr: 1790, loss: 0.093048\n","Testset: Precision: 0.812, recall: 0.878, F1-measure: 0.844, acc:0.822\n","\n","itr: 1800, loss: 0.104491\n","Testset: Precision: 0.829, recall: 0.851, F1-measure: 0.840, acc:0.822\n","\n","itr: 1810, loss: 0.091985\n","Testset: Precision: 0.818, recall: 0.851, F1-measure: 0.834, acc:0.815\n","\n","itr: 1820, loss: 0.090940\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1830, loss: 0.090477\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1840, loss: 0.089893\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1850, loss: 0.090401\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1860, loss: 0.089561\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1870, loss: 0.088970\n","Testset: Precision: 0.823, recall: 0.878, F1-measure: 0.850, acc:0.830\n","\n","itr: 1880, loss: 0.088899\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1890, loss: 0.088779\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1900, loss: 0.087847\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1910, loss: 0.087474\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1920, loss: 0.087184\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1930, loss: 0.086910\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1940, loss: 0.086713\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1950, loss: 0.086226\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1960, loss: 0.085959\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1970, loss: 0.085794\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1980, loss: 0.085672\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n","itr: 1990, loss: 0.085577\n","Testset: Precision: 0.821, recall: 0.865, F1-measure: 0.842, acc:0.822\n","\n"]}]},{"cell_type":"code","source":["print(precision_list[-1])\n","print(recall_list[-1])\n","print(f1_list[-1])\n","print(accuracy_list[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tLO1e4ukOmC","executionInfo":{"status":"ok","timestamp":1697526011442,"user_tz":240,"elapsed":28,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"66ff38ec-feff-4adc-ccde-fcb7ca6ebceb"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8205128205128205\n","0.8648648648648649\n","0.8421052631578947\n","tensor(0.8222)\n"]}]},{"cell_type":"code","source":["mlp_l1 = MLP_L1('../data/BGL/text-tfidf-template-BGL.log.structured2_l1.npz')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"YmTXjR5ybz6H","executionInfo":{"status":"error","timestamp":1697526050249,"user_tz":240,"elapsed":3115,"user":{"displayName":"Alexis Brissard","userId":"01552848406770931061"}},"outputId":"d5beac38-f19d-4944-e1ea-3d71aa89d271"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["(163, 3)\n","(163,)\n","positive: 140\n","ratio: 1.1642857142857144\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-7c1a9945ff19>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP_L1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/BGL/text-tfidf-template-BGL.log.structured2_l1.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/Code/GroupProjectDevops/log6309e-group/src/models/MLP_l1.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfea_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: scatter(): Expected dtype int64 for index"]}]},{"cell_type":"code","source":["loss_list, precision_list, recall_list, f1_list, accuracy_list = mlp.train_eval()"],"metadata":{"id":"jebm4TbLb0wK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(precision_list[-1])\n","print(recall_list[-1])\n","print(f1_list[-1])\n","print(accuracy_list[-1])"],"metadata":{"id":"LhjSmdICb6OI"},"execution_count":null,"outputs":[]}]}